{
  "annotations": { "list": [] },
  "editable": true,
  "panels": [
    {
      "title": "Inference Rate",
      "type": "stat",
      "gridPos": { "h": 6, "w": 6, "x": 0, "y": 0 },
      "targets": [
        {
          "expr": "sum(rate(ml_inference_total[5m]))",
          "legendFormat": "Inferences/s"
        }
      ],
      "fieldConfig": {
        "defaults": { "unit": "ops" }
      }
    },
    {
      "title": "Average Inference Time",
      "type": "stat",
      "gridPos": { "h": 6, "w": 6, "x": 6, "y": 0 },
      "targets": [
        {
          "expr": "histogram_quantile(0.50, sum(rate(ml_inference_duration_seconds_bucket[5m])) by (le))",
          "legendFormat": "Median"
        }
      ],
      "fieldConfig": {
        "defaults": { "unit": "s" }
      }
    },
    {
      "title": "Model Load Status",
      "type": "stat",
      "gridPos": { "h": 6, "w": 6, "x": 12, "y": 0 },
      "targets": [
        {
          "expr": "ml_models_loaded",
          "legendFormat": "{{ model }}"
        }
      ]
    },
    {
      "title": "Image Processing Queue",
      "type": "stat",
      "gridPos": { "h": 6, "w": 6, "x": 18, "y": 0 },
      "targets": [
        {
          "expr": "celery_queue_length{queue=\"image_processing\"}",
          "legendFormat": "Pending"
        }
      ]
    },
    {
      "title": "Inference Latency by Model (p50/p95/p99)",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 6 },
      "targets": [
        {
          "expr": "histogram_quantile(0.50, sum(rate(ml_inference_duration_seconds_bucket[5m])) by (le, model))",
          "legendFormat": "p50 {{ model }}"
        },
        {
          "expr": "histogram_quantile(0.95, sum(rate(ml_inference_duration_seconds_bucket[5m])) by (le, model))",
          "legendFormat": "p95 {{ model }}"
        },
        {
          "expr": "histogram_quantile(0.99, sum(rate(ml_inference_duration_seconds_bucket[5m])) by (le, model))",
          "legendFormat": "p99 {{ model }}"
        }
      ],
      "fieldConfig": {
        "defaults": { "unit": "s" }
      }
    },
    {
      "title": "Inference Throughput by Model",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 12, "x": 12, "y": 6 },
      "targets": [
        {
          "expr": "sum(rate(ml_inference_total[5m])) by (model)",
          "legendFormat": "{{ model }}"
        }
      ],
      "fieldConfig": {
        "defaults": { "unit": "ops" }
      }
    },
    {
      "title": "Classification Confidence Distribution",
      "type": "histogram",
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 14 },
      "targets": [
        {
          "expr": "sum(rate(ml_classification_confidence_bucket[5m])) by (le)",
          "legendFormat": "{{ le }}"
        }
      ]
    },
    {
      "title": "Bedrock API Calls",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 12, "x": 12, "y": 14 },
      "targets": [
        {
          "expr": "sum(rate(bedrock_api_calls_total[5m])) by (status)",
          "legendFormat": "{{ status }}"
        }
      ],
      "fieldConfig": {
        "defaults": { "unit": "ops" }
      }
    },
    {
      "title": "Image Processing Pipeline Duration",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 24, "x": 0, "y": 22 },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(image_processing_pipeline_duration_seconds_bucket[5m])) by (le, stage))",
          "legendFormat": "p95 {{ stage }}"
        }
      ],
      "fieldConfig": {
        "defaults": { "unit": "s" }
      }
    }
  ],
  "schemaVersion": 39,
  "tags": ["imagineai", "ml", "pipeline"],
  "time": { "from": "now-3h", "to": "now" },
  "title": "ImagineAI - ML Pipeline",
  "uid": "imagineai-ml-pipeline"
}
